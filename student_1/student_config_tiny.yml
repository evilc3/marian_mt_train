## Model architecture.
dec-cell: ssru
dec-cell-base-depth: 2
dec-cell-high-depth: 1
dec-depth: 2
dim-emb: 256
enc-cell: gru
enc-cell-depth: 1
enc-depth: 6
enc-type: bidirectional
tied-embeddings-all: true
transformer-decoder-autoreg: rnn
transformer-dim-ffn: 1536
transformer-ffn-activation: relu
transformer-ffn-depth: 2
transformer-guided-alignment-layer: last
transformer-heads: 8
transformer-no-projection: false
transformer-postprocess: dan
transformer-postprocess-emb: d
transformer-preprocess: ""
transformer-tied-layers:
  []
transformer-train-position-embeddings: false
type: transformer

# Training options
cost-type: ce-mean-words
max-length: 100
mini-batch: 1000
mini-batch-fit: true
maxi-batch: 1000
optimizer-delay: 2
optimizer-params:
  - 0.9
  - 0.98
  - 1e-09
sync-sgd: true
learn-rate: 0.0003
lr-report: true
lr-decay-inv-sqrt:
  - 32000
lr-warmup: 16000
label-smoothing: 0.1
clip-norm: 0
exponential-smoothing: 0.0001
disp-freq: 100
save-freq: 2ku
early-stopping: 10

####eval params 
keep-best: true
beam-size: 8
normalize: 1
valid-freq: 2ku
valid-metrics:
  - ce-mean-words
  - bleu-detok
valid-mini-batch: 16
